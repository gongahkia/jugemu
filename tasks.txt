Implement parsers/converters for common exports into canonical format (with unit tests + golden fixtures)
Add checkpoint resume support (`--resume data/checkpoints/latest.pt`) with optimizer state restoration
Add learning-rate schedule option (cosine / warmup) and gradient accumulation to support small GPUs/CPU
Add a minimal evaluation harness: "style imitation" proxy metrics (char perplexity on heldout) and qualitative prompts set
Add an overfitting warning when corpus is small (e.g., < 50k chars) and auto-reduce `--seq-len`
Add a message normalization pipeline: newline normalization, repeated whitespace collapsing option, emoji handling toggle
Add conversation-aware formatting option: include timestamps/speaker tags in the training text when available
Add retrieval prompt templates: minimal, conversation-scaffold, and "few-shot from retrieved" variants
Add retrieve-then-rerank option: initial vector top_k then rerank with a lightweight cross-encoder (optional dependency)
Make retrieval chunking configurable: per-message vs sliding window of N messages for better context
Add ingestion metadata in Chroma: source file, line number, timestamp (if known), speaker (if known)
Add robust dedupe strategy: stable IDs + optional fuzzy dedupe for near-identical messages
Add a `VectorStore` interface with two backends: Chroma (local) and Cassandra/Astra (remote)
Implement Cassandra/Astra backend using DataStax clients, including schema/table creation and vector similarity queries
Add backend selection flag: `--vector-backend chroma|cassandra` and shared config loading
Document Chroma behavior that duplicate IDs are ignored and make ingestion idempotent by design (aligns with Chroma docs)
Add a config file (`config.toml`) to avoid long CLI flags and keep runs reproducible
Add a single "train+ingest" pipeline command that runs: parse -> ingest -> train -> smoke-sample (one command, still tiny)
Add `make`/`just` shortcuts for practitioners: `make ingest`, `make train`, `make chat`
Add packaging: `pip install -e .` support and console scripts entrypoints for `jugemu-ingest`, `jugemu-train`, `jugemu-chat`
Add performance knobs: embedding batch size, max corpus size, optional smaller embedding model for speed
Add a migration strategy for vector store schema changes (version metadata + rebuild command)
Add a data export tool: dump random retrieved examples and their similarity scores for debugging
Add a `--dry-run` mode for ingestion that shows counts, dedupe rate, and example messages without writing DB
Add a `--browse` command that prints top-N most frequent characters/tokens to help pick model size/seq_len